# Resource-efficient-fine-tuning-techniques
NLP and ML have undergone a transformation in how models are trained. With the rise of pretrained models, fine-tuning for downstream tasks has become the go-to approach. Now, weâ€™re diving deeper into instruction tuningâ€”a powerful method that enhances Large Language Models (LLMs) by making them more adaptable and useful

In this journey, weâ€™ll explore resource-efficient fine-tuning techniques that even work on free T4 GPUs available in Kaggle notebooks. Among the key methods:

ðŸ”¹ Low-Rank Adaptation (LoRA) â€“ A lightweight, efficient fine-tuning technique that reduces computational overhead by constraining weight updates while maintaining performance.

ðŸ”¹ Mixed-Precision Training â€“ Boosts speed and reduces memory usage by leveraging different numerical precisions, allowing us to train larger models faster.

ðŸ”¹ Distributed Training â€“ A must for scaling large models, enabling us to leverage multiple GPUs or machines to overcome memory constraints.

By the end of this assignment, youâ€™ll gain hands-on experience with these cutting-edge techniques and be equipped to integrate them into your own ML projects.
