{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Assignment 2\n\nNLP and ML have gone through several phases of how models are trained in recent years. With the advent of diversified pretrained models, fine-tuning these models for downstream tasks has become the standard practice. In this assignment, we are about to embark on an exciting journey into the instruction tuning methodâ€”a technique designed to make LLMs more useful. The techniques we'll delve into are not only robust but also resource-efficient, allowing us to perform the task of fine-tuning even on the free T4 GPUs available in Kaggle notebooks.\n\nAmong the techniques we will explore is **Low-Rank Adaptation (LoRA)**, a method that has proven to be efficient and effective in adapting large pre-trained language models to specific tasks. LoRA is grounded in the hypothesis that updates to the weights during adaptation have a low \"intrinsic rank\", allowing us to constrain weight updates and reduce computational complexity, while preserving model performance.\n\nComplementing LoRA, we will also engage with **mixed-precision training**. This technique combines different numerical precisions to perform computations, aiming to maximize the computational power of modern GPUs. Mixed-precision training can accelerate model training, reduce memory requirements, and thus enable us to train larger, more powerful models.\n\nFinally, we will delve into **distributed training**, a must-know technique for handling very large models or datasets. With distributed training, we can leverage multiple GPUs or even multiple machines to collectively train a single model, effectively overcoming the limitations posed by the memory capacity of individual GPUs.\n\nBy the end of this assignment, you should be well-acquainted with these cutting-edge techniques and be capable of integrating them into your own machine learning projects. Let's embark on this exciting journey into the vanguard of machine learning fine-tuning methodologies!","metadata":{}},{"cell_type":"markdown","source":"### Dataset\n\nThe Stanford Alpaca dataset is a synthetic dataset developed by Stanford researchers and is part of a project that aims to build and share an instruction-following model called Alpaca. The dataset contains 52K examples used for fine-tuning the Alpaca model, with each example consisting of a unique instruction that the model should follow, an optional context or input for the task, and the corresponding output generated by the OpenAI's text-davinci-003 model. In thiss assigment, we will use an updated version - the Alpaca-GPT4 dataset - which also contains 52K instruction-following data but generated by GPT-4 with prompts in Alpaca. More information is available at the [Data release](https://github.com/tatsu-lab/stanford_alpaca/blob/main/README.md#data-release), the [Alpaca-GPT4](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM#how-good-is-the-data) and [Alpaca project page](https://crfm.stanford.edu/2023/03/13/alpaca.html).\n\n### Model\n\nThe [Qwen2.5 models](https://qwenlm.github.io/blog/qwen2.5/) were launched by Alibaba Cloud and pretrained on the latest dataset, consisting of up to 18 trillion tokens. Qwen2.5 models are designed to be more resilient to diverse system prompts, enhancing role-play implementation, condition-setting for chatbots, and supporting over 29 languages, including Vietnamese. In terms of benchmarking, Qwen2.5-72B stands out as a top-tier performer among open-source models. For example, Qwen2.5-72B achieves an MMLU score of 86.1, surpassing even larger models like LLaMA-3-405B.\n\nAs the Qwen2.5 series offers models ranging from 0.5 billion to 72 billion parameters, for this assignment, we will select Qwen2.5 1.5B, which aligns with our resource constraints.\n\nFor our purposes, we will fine-tune the Qwen2.5 1.5B model using the Alpaca-GPT4 dataset. This assignment will allow us to simulate the supervised fine-tuning phase, similar to the process employed in the development of models like ChatGPT. By leveraging the Alpaca-GPT4 dataset, we aim to enhance the Qwen2.5 1.5B's ability to follow instructions and perform tasks as specified by user\n\n\n\n\n### Initial setup\n\nTo prepare the environment for our project, please execute the commands below:","metadata":{}},{"cell_type":"markdown","source":"#### Clone and Install Libraries (Skip this cell if you have already cloned the repository)","metadata":{}},{"cell_type":"code","source":"!git clone https://github.com/vietai-courses/Advanced-NLP05.git\n%cd Advanced-NLP05/assignment_02\n!pip install -r requirements.txt","metadata":{"execution":{"iopub.status.busy":"2024-12-18T12:46:01.774836Z","iopub.execute_input":"2024-12-18T12:46:01.775208Z","iopub.status.idle":"2024-12-18T12:46:52.909218Z","shell.execute_reply.started":"2024-12-18T12:46:01.775176Z","shell.execute_reply":"2024-12-18T12:46:52.908017Z"},"scrolled":true,"trusted":true},"outputs":[{"name":"stdout","text":"Cloning into 'Advanced-NLP05'...\nremote: Enumerating objects: 26, done.\u001b[K\nremote: Counting objects: 100% (26/26), done.\u001b[K\nremote: Compressing objects: 100% (23/23), done.\u001b[K\nremote: Total 26 (delta 2), reused 26 (delta 2), pack-reused 0 (from 0)\u001b[K\nReceiving objects: 100% (26/26), 14.18 MiB | 22.10 MiB/s, done.\nResolving deltas: 100% (2/2), done.\n/kaggle/working/Advanced-NLP05/assignment_02\nCollecting git+https://github.com/huggingface/accelerate.git (from -r requirements.txt (line 7))\n  Cloning https://github.com/huggingface/accelerate.git to /tmp/pip-req-build-ydg04oin\n  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/accelerate.git /tmp/pip-req-build-ydg04oin\n  Resolved https://github.com/huggingface/accelerate.git to commit 200c9eb7833cfa505907f6f224ebf5a275aa6d92\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hCollecting git+https://github.com/huggingface/datasets.git (from -r requirements.txt (line 9))\n  Cloning https://github.com/huggingface/datasets.git to /tmp/pip-req-build-yvt4kgb5\n  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/datasets.git /tmp/pip-req-build-yvt4kgb5\n  Resolved https://github.com/huggingface/datasets.git to commit d0c152a979d91cc34b605c0298aebc650ab7dd27\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hCollecting git+https://github.com/huggingface/peft.git (from -r requirements.txt (line 14))\n  Cloning https://github.com/huggingface/peft.git to /tmp/pip-req-build-vbfn8lpk\n  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/peft.git /tmp/pip-req-build-vbfn8lpk\n  Resolved https://github.com/huggingface/peft.git to commit c1fe8105a5a4a612a6178699e1def5c66c2638d2\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hCollecting gdown (from -r requirements.txt (line 1))\n  Downloading gdown-5.2.0-py3-none-any.whl.metadata (5.8 kB)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (0.2.0)\nRequirement already satisfied: transformers>=4.28.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (4.46.3)\nCollecting loralib (from -r requirements.txt (line 4))\n  Downloading loralib-0.1.2-py3-none-any.whl.metadata (15 kB)\nCollecting bitsandbytes (from -r requirements.txt (line 5))\n  Downloading bitsandbytes-0.45.0-py3-none-manylinux_2_24_x86_64.whl.metadata (2.9 kB)\nRequirement already satisfied: appdirs in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 6)) (1.4.4)\nCollecting einops (from -r requirements.txt (line 10))\n  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\nCollecting auto-gptq (from -r requirements.txt (line 11))\n  Downloading auto_gptq-0.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\nCollecting optimum (from -r requirements.txt (line 13))\n  Downloading optimum-1.23.3-py3-none-any.whl.metadata (20 kB)\nRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from gdown->-r requirements.txt (line 1)) (4.12.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from gdown->-r requirements.txt (line 1)) (3.15.1)\nRequirement already satisfied: requests[socks] in /opt/conda/lib/python3.10/site-packages (from gdown->-r requirements.txt (line 1)) (2.32.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from gdown->-r requirements.txt (line 1)) (4.66.4)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.28.0->-r requirements.txt (line 3)) (0.26.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.28.0->-r requirements.txt (line 3)) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.28.0->-r requirements.txt (line 3)) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.28.0->-r requirements.txt (line 3)) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.28.0->-r requirements.txt (line 3)) (2024.5.15)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.28.0->-r requirements.txt (line 3)) (0.20.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.28.0->-r requirements.txt (line 3)) (0.4.5)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes->-r requirements.txt (line 5)) (2.4.0)\nRequirement already satisfied: typing_extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from bitsandbytes->-r requirements.txt (line 5)) (4.12.2)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==1.2.0.dev0->-r requirements.txt (line 7)) (5.9.3)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==3.2.1.dev0->-r requirements.txt (line 9)) (17.0.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets==3.2.1.dev0->-r requirements.txt (line 9)) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==3.2.1.dev0->-r requirements.txt (line 9)) (2.2.3)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==3.2.1.dev0->-r requirements.txt (line 9)) (3.4.1)\nRequirement already satisfied: multiprocess<0.70.17 in /opt/conda/lib/python3.10/site-packages (from datasets==3.2.1.dev0->-r requirements.txt (line 9)) (0.70.16)\nRequirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets==3.2.1.dev0->-r requirements.txt (line 9)) (2024.6.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==3.2.1.dev0->-r requirements.txt (line 9)) (3.9.5)\nCollecting rouge (from auto-gptq->-r requirements.txt (line 11))\n  Downloading rouge-1.0.1-py3-none-any.whl.metadata (4.1 kB)\nCollecting gekko (from auto-gptq->-r requirements.txt (line 11))\n  Downloading gekko-1.2.1-py3-none-any.whl.metadata (3.0 kB)\nCollecting coloredlogs (from optimum->-r requirements.txt (line 13))\n  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from optimum->-r requirements.txt (line 13)) (1.13.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==3.2.1.dev0->-r requirements.txt (line 9)) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==3.2.1.dev0->-r requirements.txt (line 9)) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==3.2.1.dev0->-r requirements.txt (line 9)) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==3.2.1.dev0->-r requirements.txt (line 9)) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==3.2.1.dev0->-r requirements.txt (line 9)) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==3.2.1.dev0->-r requirements.txt (line 9)) (4.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers>=4.28.0->-r requirements.txt (line 3)) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown->-r requirements.txt (line 1)) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown->-r requirements.txt (line 1)) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown->-r requirements.txt (line 1)) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown->-r requirements.txt (line 1)) (2024.6.2)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes->-r requirements.txt (line 5)) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes->-r requirements.txt (line 5)) (3.1.4)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->gdown->-r requirements.txt (line 1)) (2.5)\nCollecting humanfriendly>=9.1 (from coloredlogs->optimum->-r requirements.txt (line 13))\n  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==3.2.1.dev0->-r requirements.txt (line 9)) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==3.2.1.dev0->-r requirements.txt (line 9)) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==3.2.1.dev0->-r requirements.txt (line 9)) (2024.1)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown->-r requirements.txt (line 1)) (1.7.1)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from rouge->auto-gptq->-r requirements.txt (line 11)) (1.16.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->optimum->-r requirements.txt (line 13)) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes->-r requirements.txt (line 5)) (2.1.5)\nDownloading gdown-5.2.0-py3-none-any.whl (18 kB)\nDownloading loralib-0.1.2-py3-none-any.whl (10 kB)\nDownloading bitsandbytes-0.45.0-py3-none-manylinux_2_24_x86_64.whl (69.1 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading einops-0.8.0-py3-none-any.whl (43 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading auto_gptq-0.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23.5 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m23.5/23.5 MB\u001b[0m \u001b[31m76.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading optimum-1.23.3-py3-none-any.whl (424 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m424.1/424.1 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading gekko-1.2.1-py3-none-any.whl (13.2 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m101.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading rouge-1.0.1-py3-none-any.whl (13 kB)\nDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: accelerate, datasets, peft\n  Building wheel for accelerate (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for accelerate: filename=accelerate-1.2.0.dev0-py3-none-any.whl size=336425 sha256=6e83e836d6bb42f672e608c5fe6606d0f7b25d16df0a3a92312b44d63dd5f33f\n  Stored in directory: /tmp/pip-ephem-wheel-cache-7xghg7eb/wheels/9c/a3/1e/47368f9b6575655fe9ee1b6350cfa7d4b0befe66a35f8a8365\n  Building wheel for datasets (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for datasets: filename=datasets-3.2.1.dev0-py3-none-any.whl size=480709 sha256=17a9c122880751ad4b9b2a8f374774a04e6dd2e5ee751520a76be04004a2daac\n  Stored in directory: /tmp/pip-ephem-wheel-cache-7xghg7eb/wheels/57/f4/c4/53c677af89fec0ef3226c1e75a38367b37c2fa626f0544d3e4\n  Building wheel for peft (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for peft: filename=peft-0.14.1.dev0-py3-none-any.whl size=375586 sha256=5b0f23497c20809f293387bb0ea46057787a20a2024793f1efd959ab8d3b0676\n  Stored in directory: /tmp/pip-ephem-wheel-cache-7xghg7eb/wheels/d7/c7/de/1368fac8590e1b103ddc2ec2a28ad51d83aded1a3830e8a087\nSuccessfully built accelerate datasets peft\nInstalling collected packages: rouge, loralib, humanfriendly, gekko, einops, coloredlogs, gdown, bitsandbytes, accelerate, datasets, peft, optimum, auto-gptq\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 1.1.1\n    Uninstalling accelerate-1.1.1:\n      Successfully uninstalled accelerate-1.1.1\n  Attempting uninstall: datasets\n    Found existing installation: datasets 3.1.0\n    Uninstalling datasets-3.1.0:\n      Successfully uninstalled datasets-3.1.0\nSuccessfully installed accelerate-1.2.0.dev0 auto-gptq-0.7.1 bitsandbytes-0.45.0 coloredlogs-15.0.1 datasets-3.2.1.dev0 einops-0.8.0 gdown-5.2.0 gekko-1.2.1 humanfriendly-10.0 loralib-0.1.2 optimum-1.23.3 peft-0.14.1.dev0 rouge-1.0.1\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"#### RUN ONLY AS NEEDED (Execute this code if you have recently restarted the kernel).\n\nThe following instructions are intended for situations where you encounter an Out of Memory error and need to restart the kernel. After restarting the kernel, execute the code in this cell to resume operations smoothly. Therefore, this should only be run if you've recently restarted the kernel.","metadata":{}},{"cell_type":"code","source":"%cd Advanced-NLP05/assignment_02","metadata":{"execution":{"iopub.status.busy":"2024-12-18T12:46:52.911127Z","iopub.execute_input":"2024-12-18T12:46:52.911435Z","iopub.status.idle":"2024-12-18T12:46:52.917012Z","shell.execute_reply.started":"2024-12-18T12:46:52.911407Z","shell.execute_reply":"2024-12-18T12:46:52.916005Z"},"trusted":true},"outputs":[{"name":"stdout","text":"[Errno 2] No such file or directory: 'Advanced-NLP05/assignment_02'\n/kaggle/working/Advanced-NLP05/assignment_02\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## Part 1: Low-Rank Adaptation of Large Language Models for Efficient Fine-tuning","metadata":{}},{"cell_type":"markdown","source":"![](https://miro.medium.com/v2/resize:fit:730/1*D_i25E9dTd_5HMa45zITSg.png)\n\nFigure 1: LoRA method. We only train A and B.\n\n\n### 1. Introduction\n\n**LoRA, or Low-Rank Adaptation**, is a technique for adapting large language models to specific tasks or domains more efficiently. It's based on the observation that as models get larger, the conventional approach of full fine-tuning becomes less feasible due to the large number of parameters involved.\n\nThis process involves injecting the matrices into the dense layer's update, optimizing them for the specific adaptation task while the original pretrained model weights remain unchanged.\n\nHere are some of the key points of the LoRA technique:\n\n- **Freezing Pretrained Weights**: Instead of modifying all the parameters of a pretrained model during fine-tuning, LoRA freezes the pretrained weights. This means that the original model weights remain unchanged during the adaptation process.\n\n- **Rank Decomposition Matrices**: LoRA freezes the pretrained model weights and *injects trainable rank decomposition matrices* into each layer of the Transformer architecture. These matrices are used to adjust the output of each layer in a way that's specific to the adaptation task.\n\n- **Indirect Training of Dense Layers**: The rank decomposition matrices allow for the indirect training of each dense layer in the neural network. They are injected into the layer's update during the adaptation process and optimized to enhance the layer's performance on the specific task or domain.\n\n- **Significant Reduction in Trainable Parameters**: By focusing on these rank decomposition matrices instead of the entire set of model weights, LoRA greatly reduces the number of trainable parameters for downstream tasks. For instance, in the case of GPT-3, LoRA can reduce the number of trainable parameters by a factor of 10,000.\n\n- **Maintaining Model Performance**: Despite the significant reduction in the number of trainable parameters, the LoRA technique is designed to maintain or even improve the performance of the large language model on the specific task or domain.\n\nIn summary, LoRA is a method that tackles the challenge of adapting large language models to specific tasks or domains in a more efficient and feasible way, making the fine-tuning process more manageable and less resource-intensive.","metadata":{}},{"cell_type":"markdown","source":"### 2. Details\n\nThe LoRA technique introduces a mathematical concept known as low-rank approximation into the fine-tuning process of large language models. Here's a mathematical description of the process:\n\nLoRA involves modifying the pre-trained weight matrix $\\mathbf{W}_0 \\in \\mathbb{R}^{d \\times k}$ of a neural network layer by introducing a low-rank parametrized update matrix $\\Delta \\mathbf{W} = \\mathbf{B}\\mathbf{A}$, where $\\mathbf{B} \\in \\mathbb{R}^{d \\times r}$, $\\mathbf{A} \\in \\mathbb{R}^{r \\times k}$, and $r \\ll \\min(d, k)$.\n\nDuring the adaptation process, $\\mathbf{W}_0$ is kept frozen, which means it does not receive any gradient updates. The trainable parameters are contained within $\\mathbf{A}$ and $\\mathbf{B}$, which form the low-rank update matrix $\\Delta \\mathbf{W}$.\n\nIt's important to note that both $\\mathbf{W}_0$ and $\\Delta \\mathbf{W} = \\mathbf{B}\\mathbf{A}$ are multiplied with the same input, and their respective output vectors are summed. If $\\mathbf{x}$ is the input and $\\mathbf{h} = \\mathbf{W}_0\\mathbf{x}$ is the output of the original weight matrix, the modified output is:\n\n$\\mathbf{h} = \\mathbf{W}_0\\mathbf{x} + \\Delta \\mathbf{W} \\mathbf{x} = \\mathbf{W}_0\\mathbf{x} + \\mathbf{B}\\mathbf{A}\\mathbf{x} = (\\mathbf{W}_0 + \\mathbf{B}\\mathbf{A})\\mathbf{x}$ \n\nin which $\\mathbf{W}_0 + \\mathbf{B}\\mathbf{A}$ is called **merge** operation. We will implement it in this assignment. \n\nAt the beginning of training, we initialize $\\mathbf{A}$ with a random Gaussian distribution and $\\mathbf{B}$ with zero, such that $\\Delta \\mathbf{W} = \\mathbf{B}\\mathbf{A}$ is zero, as shown in **Figure 1**. This ensures that the initial output of the model remains the same as in the pre-training phase, and the adaptation starts from the original model state. \n\nThe low-rank update $\\Delta \\mathbf{W} = \\mathbf{B}\\mathbf{A}$ then evolves during training, helping to specialize the model for a specific task while keeping the number of trainable parameters manageable. Additionally, $\\Delta \\mathbf{W}$ is scaled by $\\frac{\\alpha}{r}$ where $\\alpha$ is a constant hyper-parameter. **(*)**\n\nThis process is applied for each Linear layer of self-attention layer in the Qwen2.5 language model, leading to an adapted model that's specialized for a specific task or domain, with significantly fewer trainable parameters than the original model. For example, with GPT-3 175B, VRAM consumption during training is reduced from 1.2TB to 350GB. If $r = 4$ and only the query and value projection matrices are adapted, the checkpoint size is reduced by approximately 10,000 times (from 350GB to 35MB). This allows training with significantly fewer GPUs and helps to avoid communication overhead.\n\nAnother benefit is the ability to switch between tasks at a lower cost by only swapping the LoRA weights, as opposed to all parameters. This enables the creation of many customized models that can be swapped in and out on the fly on machines that store the pre-trained weights in VRAM.","metadata":{}},{"cell_type":"markdown","source":"**(*)** The reason for scaling the update $\\Delta W x$ by $\\frac{\\alpha}{r}$ is primarily for easier optimization.\n\nConsider the scenario where the rank $r$ changes during training. If you were to increase or decrease $r$, without this scaling factor, it would significantly affect the magnitude of the weight updates and thereby the learning dynamics of the model. In other words, changing $r$ would mean that you need to retune the learning rate or other hyperparameters, which is a laborious and time-consuming task.\n\nBy scaling the updates by $\\frac{\\alpha}{r}$, the authors make the learning process more robust to changes in $r$. $\\alpha$ is a constant, so this scaling factor effectively normalizes the magnitude of the updates relative to the rank of the low-rank approximation.\n\nThis way, even when $r$ changes, the overall scale of the updates remains approximately constant, meaning you can use the same learning rate and other hyperparameters. This is advantageous because it makes the training process more efficient and less sensitive to the choice of $r$.\n\nKeep in mind that this is a heuristic and it may not always provide the optimal solution for every problem or dataset, but it is a practical choice that often works well in practice.","metadata":{}},{"cell_type":"markdown","source":"### 3. Implementation\n\nLet's break down the code, please take a look at the `lora_layer.py` file. The main components are:\n\n- **LoraLayer** class: This is a base class that provides common functionality for both linear and embedding layers using the LoRA technique. It keeps track of LoRA parameters including the rank **r** and two sets of weights **lora_A** and **lora_B** (or **lora_embedding_A** and **lora_embedding_B** for the embedding layer). Two methods, **update_layer** and **update_layer_embedding**, are defined to update these parameters for linear and embedding layers, respectively.\n\n- **Linear** and **Embedding** classes: These classes extend their corresponding PyTorch classes (**nn.Linear** and **nn.Embedding**) and the **LoraLayer** class. They initialize their superclasses as well as the LoRA parameters, and overwrite the **merge**, **unmerge**, and **forward** methods to implement the LoRA technique. The **merge** method combines the original weights of the layer with the LoRA weights, and **unmerge** undoes this operation. The **forward** method applies the layer operation either with or without the LoRA technique, depending on whether LoRA is enabled.\n\nThis assignment is heavily based on the internal codebase of ðŸ¤— PEFT library. ðŸ¤— PEFT, or **Parameter-Efficient Fine-Tuning (PEFT)**, is a library for efficiently adapting pre-trained language models (PLMs) to various downstream applications without fine-tuning all the modelâ€™s parameters. Recent state-of-the-art PEFT techniques achieve performance comparable to that of full fine-tuning.\n\nIf you are new to PEFT, get started by reading the [Quicktour](https://huggingface.co/docs/peft/quicktour) guide and conceptual guides for [LoRA](https://huggingface.co/docs/peft/main/en/conceptual_guides/lora) methods.\n\n\n**!!!** If you want to modify the .py files in Kaggle, you may need to use [Magic Commands](https://www.kaggle.com/code/matinmahmoudi/complete-guide-to-magic-commands-a-to-z) such as **%load**, **%%writefile**.","metadata":{}},{"cell_type":"markdown","source":"#### Q1: Implement the Merge operation in LoRA (15 points)\nIn the provided `lora_layer.py` file, your task is to complete the `merge` method within the `Linear` class. As a useful reference, consider the already implemented `merge` method in the `Embedding` class. This should provide a clear guide on how to approach this task.\n\n#### Q2: Implement the Forward Pass in LoRA (15 points)\nIn the provided `lora_layer.py` file, your task is to complete the `forward` method within the `Linear` class. As a useful reference, consider the already implemented `forward` method in the `Embedding` class. This should provide a clear guide on how to approach this task.\n\n#### Q3: Construct the LoRA Model and Dataloaders for Training (20 points)\nIn the provided `train.py` file, your task is to complete the `load_pretrained_model` function and `prepare_dataloader`. The aforementioned PEFT's Quicktour guide and LoRA's conceptual guide can be useful references. Note, the specific details related to distributed training can be overlooked at this stage.\n\nOnce you've finished the implementation, it's time to train your LoRA model. Congratulations!","metadata":{}},{"cell_type":"code","source":"# %load lora_layer.py\nimport math\nimport warnings\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# The base class for LoRA layers\nclass LoraLayer:\n    def __init__(\n        self,\n        in_features: int,  # The number of input features\n        out_features: int,  # The number of output features\n    ):\n        # Initializes dictionaries to store various parameters for each adapter in the layer\n        self.r = {}  # The rank of the low-rank matrix\n        self.lora_alpha = {}  # The scaling factor\n        self.scaling = {}  # The calculated scaling factor (lora_alpha / r)\n\n        # Dropout layers for each adapter\n        self.lora_dropout = nn.ModuleDict({})\n\n        # Weight matrices for the linear layers\n        self.lora_A = nn.ModuleDict({})\n        self.lora_B = nn.ModuleDict({})\n\n        # Weight matrices for the embedding layers\n        self.lora_embedding_A = nn.ParameterDict({})\n        self.lora_embedding_B = nn.ParameterDict({})\n\n        # Boolean flag indicating whether the weights have been merged\n        self.merged = False\n\n        # Boolean flag indicating whether the adapters are disabled\n        self.disable_adapters = False\n\n        # Stores the number of input and output features\n        self.in_features = in_features\n        self.out_features = out_features\n    \n    # Method to update the parameters of the layer with a new adapter\n    def update_layer(self, adapter_name, r, lora_alpha, lora_dropout, init_lora_weights):\n        # Updates the rank and scaling factor for the adapter\n        self.r[adapter_name] = r\n        self.lora_alpha[adapter_name] = lora_alpha\n\n        # If dropout rate is greater than 0, creates a dropout layer, otherwise creates an identity layer\n        if lora_dropout > 0.0:\n            lora_dropout_layer = nn.Dropout(p=lora_dropout)\n        else:\n            lora_dropout_layer = nn.Identity()\n\n        # Updates the dropout layer for the adapter\n        self.lora_dropout.update(nn.ModuleDict({adapter_name: lora_dropout_layer}))\n\n        # If rank is greater than 0, creates trainable parameters for the adapter\n        if r > 0:\n            self.lora_A.update(nn.ModuleDict({adapter_name: nn.Linear(self.in_features, r, bias=False)}))\n            self.lora_B.update(nn.ModuleDict({adapter_name: nn.Linear(r, self.out_features, bias=False)}))\n            self.scaling[adapter_name] = lora_alpha / r\n\n        # If init_lora_weights is True, resets the parameters of the adapter\n        if init_lora_weights:\n            self.reset_lora_parameters(adapter_name)\n\n        # Moves the layer to the same device as the weight tensor\n        self.to(self.weight.device)\n\n     # Method to update the parameters of the embedding layer with a new adapter\n    def update_layer_embedding(self, adapter_name, r, lora_alpha, lora_dropout, init_lora_weights):\n        # Updates the rank and scaling factor for the adapter\n        self.r[adapter_name] = r\n        self.lora_alpha[adapter_name] = lora_alpha\n\n        # If dropout rate is greater than 0, creates a dropout layer, otherwise creates an identity layer\n        if lora_dropout > 0.0:\n            lora_dropout_layer = nn.Dropout(p=lora_dropout)\n        else:\n            lora_dropout_layer = nn.Identity()\n\n        # Updates the dropout layer for the adapter\n        self.lora_dropout.update(nn.ModuleDict({adapter_name: lora_dropout_layer}))\n\n        # If rank is greater than 0, creates trainable parameters for the adapter\n        if r > 0:\n            self.lora_embedding_A.update(\n                nn.ParameterDict({adapter_name: nn.Parameter(self.weight.new_zeros((r, self.in_features)))})\n            )\n            self.lora_embedding_B.update(\n                nn.ParameterDict({adapter_name: nn.Parameter(self.weight.new_zeros((self.out_features, r)))})\n            )\n            self.scaling[adapter_name] = lora_alpha / r\n\n        # If init_lora_weights is True, resets the parameters of the adapter\n        if init_lora_weights:\n            self.reset_lora_parameters(adapter_name)\n\n        # Moves the layer to the same device as the weight tensor\n        self.to(self.weight.device)\n\n    # Method to reset the parameters of an adapter\n    def reset_lora_parameters(self, adapter_name):\n        if adapter_name in self.lora_A.keys():\n            # initialize A the same way as the default for nn.Linear and B to zero\n            nn.init.kaiming_uniform_(self.lora_A[adapter_name].weight, a=math.sqrt(5))\n            nn.init.zeros_(self.lora_B[adapter_name].weight)\n        if adapter_name in self.lora_embedding_A.keys():\n            # initialize a the same way as the default for nn.linear and b to zero\n            nn.init.zeros_(self.lora_embedding_A[adapter_name])\n            nn.init.normal_(self.lora_embedding_B[adapter_name])\n\n# LoRA implemented in an Embedding layer\nclass Embedding(nn.Embedding, LoraLayer):\n    \"\"\"\n    The Embedding class is an extension of the PyTorch nn.Embedding class \n    and LoraLayer class to incorporate the LoRA method.\n    \"\"\"\n    def __init__(\n        self,\n        adapter_name: str,\n        num_embeddings: int,\n        embedding_dim: int,\n        r: int = 0,\n        lora_alpha: int = 1,\n        lora_dropout: float = 0.0,\n        **kwargs,\n    ):\n        # Pop the init_lora_weights flag from kwargs\n        init_lora_weights = kwargs.pop(\"init_lora_weights\", True)\n\n        # Call the constructors of the parent classes\n        nn.Embedding.__init__(self, num_embeddings, embedding_dim, **kwargs)\n        LoraLayer.__init__(self, in_features=num_embeddings, out_features=embedding_dim)\n\n        # Freezing the pre-trained weight matrix\n        self.weight.requires_grad = False\n\n        # Reset the parameters of the Embedding layer and update it with the adapter\n        nn.Embedding.reset_parameters(self)\n        self.update_layer_embedding(adapter_name, r, lora_alpha, lora_dropout, init_lora_weights)\n\n        # Set the active adapter\n        self.active_adapter = adapter_name\n\n    # Separate low-rank approximation from original weight\n    def unmerge(self, mode: bool = True):\n        # If the weights are already unmerged, raise a warning\n        if not self.merged:\n            warnings.warn(\"Already unmerged. Nothing to do.\")\n            return\n        # If the rank of the active adapter is greater than 0, subtract the product of the LoRA weights\n        # from the weights of the embedding\n        if self.r[self.active_adapter] > 0:\n            self.weight.data -= (\n                transpose(\n                    self.lora_embedding_B[self.active_adapter] @ self.lora_embedding_A[self.active_adapter], True\n                )\n                * self.scaling[self.active_adapter]\n            )\n            self.merged = False\n\n    # Merge low-rank approximation with original weights\n    def merge(self):\n        # If the weights are already merged, raise a warning\n        if self.merged:\n            warnings.warn(\"Already merged. Nothing to do.\")\n            return\n        # If the rank of the active adapter is greater than 0, add the product of the LoRA weights\n        # to the weights of the embedding\n        if self.r[self.active_adapter] > 0:\n            self.weight.data += (\n                transpose(\n                    self.lora_embedding_B[self.active_adapter] @ self.lora_embedding_A[self.active_adapter], True\n                )\n                * self.scaling[self.active_adapter]\n            )\n            self.merged = True\n\n    # Defines the computation performed at every call.\n    def forward(self, x: torch.Tensor):\n        # If adapters are disabled and there is an active adapter with rank > 0 and it is merged\n        # Subtract the LoRA weights from the original weights and set merged to False\n        if self.disable_adapters:\n            if self.r[self.active.adapter] > 0 and self.merged:\n                self.weight.data -= (\n                    transpose(\n                        self.lora_embedding_B[self.active_adapter].weight\n                        @ self.lora_embedding_A[self.active_adapter].weight,\n                        True,\n                    )\n                    * self.scaling[self.active_adapter]\n                )\n                self.merged = False\n            # Forward pass with the original weights\n            return nn.Embedding.forward(self, x)\n\n        # If there is an active adapter with rank > 0 and it is not merged\n        elif self.r[self.active_adapter] > 0 and not self.merged:\n            result = nn.Embedding.forward(self, x)\n            # Compute the forward pass with the LoRA weights and add it to the result\n            if self.r[self.active_adapter] > 0:\n                after_A = F.embedding(\n                    x,\n                    self.lora_embedding_A[self.active_adapter].T,\n                    self.padding_idx,\n                    self.max_norm,\n                    self.norm_type,\n                    self.scale_grad_by_freq,\n                    self.sparse,\n                )\n                result += (after_A @ self.lora_embedding_B[self.active_adapter].T) * self.scaling[self.active_adapter]\n            return result\n        else:\n            return nn.Embedding.forward(self, x)\n\n\n# Lora is implemented in a dense (Linear) layer\nclass Linear(nn.Linear, LoraLayer):\n    \n    def __init__(\n        self,\n        adapter_name: str,\n        in_features: int,\n        out_features: int,\n        r: int = 0,\n        lora_alpha: int = 1,\n        lora_dropout: float = 0.0,\n        fan_in_fan_out: bool = False,  # Set this to True if the layer to replace stores weight like (fan_in, fan_out)\n        **kwargs,\n    ):\n        # Initialize weights for LoRA layer\n        init_lora_weights = kwargs.pop(\"init_lora_weights\", True)\n\n        # Initialize linear and LoRA layers\n        nn.Linear.__init__(self, in_features, out_features, **kwargs)\n        LoraLayer.__init__(self, in_features=in_features, out_features=out_features)\n\n        # Freezing the pre-trained weight matrix\n        self.weight.requires_grad = False\n\n        # Transpose the weight if the layer to replace stores weight like (fan_in, fan_out)\n        self.fan_in_fan_out = fan_in_fan_out\n        if fan_in_fan_out:\n            self.weight.data = self.weight.data.T\n\n        # Reset linear layer parameters and update LoRA layer\n        nn.Linear.reset_parameters(self)\n        self.update_layer(adapter_name, r, lora_alpha, lora_dropout, init_lora_weights)\n        self.active_adapter = adapter_name\n\n    def merge(self):\n        # Merge low-rank approximation with original weights\n        if self.active_adapter not in self.lora_A.keys():\n            return\n        if self.merged:\n            warnings.warn(\"Already merged. Nothing to do.\")\n            return\n        if self.r[self.active_adapter] > 0:\n            # TODO: Merge the LoRA parameters by adding the product of lora_B weights and lora_A weights (after transposing \n            # if necessary) to the original weights, scaled by the LoRA scaling factor. After this operation, set the merged\n            # flag to True.\n    \n    # the following is my answer to Question 1:\n            \n            #BEGIN CODE:\n            # Add the scaled product of LoRA weights to the original weight\n            self.weight.data += (\n            transpose(self.lora_B[self.active_adapter].weight @ self.lora_A[self.active_adapter].weight, \n                      self.fan_in_fan_out) * self.scaling[self.active_adapter]\n        )\n            self.merged = True \n            #END CODE    \n\n    def unmerge(self):\n        # Separate low-rank approximation from original weights\n        if self.active_adapter not in self.lora_A.keys():\n            return\n        if not self.merged:\n            warnings.warn(\"Already unmerged. Nothing to do.\")\n            return\n        if self.r[self.active_adapter] > 0:\n            self.weight.data -= (\n                transpose(\n                    self.lora_B[self.active_adapter].weight @ self.lora_A[self.active_adapter].weight,\n                    self.fan_in_fan_out,\n                )\n                * self.scaling[self.active_adapter]\n            )\n            self.merged = False\n\n    def forward(self, x: torch.Tensor):\n        previous_dtype = x.dtype\n        if self.active_adapter not in self.lora_A.keys():\n            return F.linear(x, transpose(self.weight, self.fan_in_fan_out), bias=self.bias)\n        if self.disable_adapters:\n            if self.r[self.active_adapter] > 0 and self.merged:\n                self.unmerge()\n            result = F.linear(x, transpose(self.weight, self.fan_in_fan_out), bias=self.bias)\n        elif self.r[self.active_adapter] > 0 and not self.merged:\n            result = F.linear(x, transpose(self.weight, self.fan_in_fan_out), bias=self.bias)\n            # Changing data type for ensuring consistency\n            x = x.to(self.lora_A[self.active_adapter].weight.dtype)\n            \n            # TODO: If the LoRA adapter is active and not merged, add the output of the LoRA layers to the result. This involves\n            # passing the input through lora_A, applying dropout, then passing it through lora_B. The output is scaled by the\n            # LoRA scaling factor and added to the result.\n\n    # the following is my answer to Question 2:\n            \n            #BEGIN CODE:\n            lora_output = self.lora_B[self.active_adapter](\n            self.lora_dropout[self.active_adapter](self.lora_A[self.active_adapter](x))\n        )\n            result += lora_output * self.scaling[self.active_adapter]   \n            #END CODE  \n        else:\n            result = F.linear(x, transpose(self.weight, self.fan_in_fan_out), bias=self.bias)\n        \n        # Reverting to the previous data type\n        result = result.to(previous_dtype)\n        return result\n    \ndef transpose(weight, fan_in_fan_out):\n    # Helper function to transpose weights if required\n    return weight.T if fan_in_fan_out else weight\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T12:46:52.918478Z","iopub.execute_input":"2024-12-18T12:46:52.918805Z","iopub.status.idle":"2024-12-18T12:46:52.951931Z","shell.execute_reply.started":"2024-12-18T12:46:52.918770Z","shell.execute_reply":"2024-12-18T12:46:52.951089Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# %load train.py\nimport os\nimport torch\nfrom tqdm import tqdm\n\nfrom peft import get_peft_model, LoraConfig\nfrom transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, DataCollatorForSeq2Seq\n\nfrom contextlib import nullcontext\n\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.distributed import init_process_group, destroy_process_group\nimport torch.distributed as dist\nfrom torch.utils.data.distributed import DistributedSampler\nfrom torch.utils.data import DataLoader, SequentialSampler\n\n\nfrom lora_model import LoraModelForCasualLM\nfrom utils.common import download_from_driver\nfrom prepare_data import create_datasets\n\nimport warnings\nwarnings.filterwarnings('ignore')\ntorch.manual_seed(42)\ntorch.backends.cudnn.deterministic = True\n\n\nclass Trainer:\n    def __init__(self,\n                 model,\n                 tokenizer,\n                 gpu_id: int,\n                 is_ddp_training: bool = True,\n                 output_dir: str = 'checkpoints/',\n                 num_epochs: int = 10,\n                 max_length: int = 128,\n                 batch_size: int = 8,\n                 mixed_precision_dtype=None,\n                 gradient_accumulation_steps: int = 16):\n        \"\"\"\n        Initialize the Trainer class.\n\n        Args:\n            model: Pretrained model object.\n            tokenizer: Tokenizer object for text processing.\n            num_epochs: Number of training epochs.\n            max_length: Maximum sequence length.\n            batch_size: Training batch size.\n            gpu_id: GPU ID for training.\n        \"\"\"\n\n        self.num_epochs = num_epochs\n        self.max_length = max_length\n        self.batch_size = batch_size\n        self.output_dir = output_dir\n        self.tokenizer = tokenizer\n        self.is_ddp_training = is_ddp_training\n\n        self.gpu_id = gpu_id\n        self.model = model.to(f\"cuda:{self.gpu_id}\")\n        self.gradient_accumulation_steps = gradient_accumulation_steps\n\n        self.mixed_precision_dtype = mixed_precision_dtype\n        self.ctx = None\n        self.gradscaler = None\n\n        # set mixed precision context\n        self.set_mixed_precision_context(mixed_precision_dtype)\n\n    def set_mixed_precision_context(self, mixed_precision_dtype):\n        \n        # TODO: Setup mixed precision training context\n        if mixed_precision_dtype is None:\n            # If 'mixed_precision_dtype' is None, use 'nullcontext',\n            self.ctx = nullcontext()\n        else:\n            # TODO Otherwise, use 'torch.amp.autocast' context with the specified dtype, and initialize GradScaler if mixed_precision_dtype is float16.\n            # Submission Code\n            self.ctx = torch.cuda.amp.autocast(dtype=mixed_precision_dtype)\n            if mixed_precision_dtype == torch.float16:\n                self.gradscaler = torch.cuda.amp.GradScaler()\n        \n\n\n    def _set_ddp_training(self):\n        # TODO: Initialize the DistributedDataParallel wrapper for the model.\n        # You would need to pass the model and specify the device IDs\n        # and output device for the data parallelism.\n        self.model = DDP(self.model, device_ids=[self.gpu_id], output_device=self.gpu_id) #Submission Code\n\n\n    def _run_batch(self, batch):\n        \"\"\"\n        Run a single training batch.\n\n        Args:\n            batch: Batch data.\n\n        Returns:\n            Loss value for the batch.\n        \"\"\"\n\n        with self.ctx:\n            outputs = self.model(**batch)\n            loss = outputs.loss / self.gradient_accumulation_steps  # Normalize loss\n        loss_val = loss.item()\n\n        # TODO: If 'mixed_precision_dtype' is torch.float16, you have to modify the backward using the gradscaler.\n        if self.mixed_precision_dtype == torch.float16:\n            self.gradscaler.scale(loss).backward() #Submission code\n\n        else:\n            loss.backward()\n\n        return loss_val\n\n    def _run_epoch(self, train_dataloader, epoch):\n        \"\"\"\n        Run a single training epoch.\n\n        Args:\n            train_loader: Training data loader.\n            epoch: Current epoch number.\n\n        Returns:\n            Total loss value for the epoch.\n        \"\"\"\n\n        epoch_loss = 0\n        self.model.train()\n\n        if _is_master_process():\n            train_progress_bar = tqdm(\n                train_dataloader, desc=f\"Epoch {epoch + 1} [Training]\", position=0, leave=False)\n        else:\n            train_progress_bar = train_dataloader\n\n        # Add counter for gradient accumulation\n        steps = 0\n        self.optimizer.zero_grad()  # Reset gradients at the beginning of each epoch\n        for step, batch in enumerate(train_progress_bar):\n            steps += 1\n            batch = {key: value.to(self.gpu_id)\n                     for key, value in batch.items()}\n            loss = self._run_batch(batch)\n            epoch_loss += loss\n\n            # Perform optimizer step and reset gradients after accumulating enough gradients\n            if steps % self.gradient_accumulation_steps == 0:\n\n                # If 'mixed_precision_dtype' is torch.float16, you have to modify the gradient update step using the gradscaler.\n                if self.mixed_precision_dtype==torch.float16:\n                    self.gradscaler.step(self.optimizer) #submission code for optimizer step\n                    self.gradscaler.update () #submission code for updating scaler factor\n\n                else:\n                    self.optimizer.step()\n                self.optimizer.zero_grad()\n\n                torch.cuda.empty_cache()\n        epoch_loss /= (len(train_dataloader) /\n                       self.gradient_accumulation_steps)\n        return epoch_loss\n\n    def _save_checkpoint(self, epoch):\n        path_dir = f\"{self.output_dir}/epoch_{epoch}\"\n\n        # check path_dir exited\n        if not os.path.exists(path_dir):\n            os.makedirs(path_dir)\n\n        # save checkpoints\n        if self.is_ddp_training and _is_master_process():\n            self.model.module.save_pretrained(f'epoch_{epoch}_checkpoint')\n        else:\n            self.model.save_pretrained(f'epoch_{epoch}_checkpoint')\n\n        print(\"Done saved at\", f'epoch_{epoch}_checkpoint')\n\n    def prepare_dataloader(self, train_dataset, eval_dataset):\n\n        # TODO: Prepare the training DataLoader. Initialize 'DataLoader' with 'train_dataset'\n        # and the appropriate 'batch_size'.\n        # Depending on whether the training is distributed (is_ddp_training),\n        # use 'DistributedSampler' for 'sampler' argument, else use 'None'.\n        # Use 'DataCollatorForSeq2Seq' for 'collate_fn', passing 'tokenizer', padding settings and pad_to_multiple_of to 8, and return_tensors=\"pt\"\n        # Also add drop_last to True.\n\n        data_trainloader = DataLoader(\n        train_dataset,\n        batch_size=self.batch_size,\n        sampler=DistributedSampler(train_dataset) if self.is_ddp_training else None,\n        collate_fn=DataCollatorForSeq2Seq(\n            tokenizer=self.tokenizer, padding=True, pad_to_multiple_of=8, return_tensors=\"pt\"\n        ),\n        drop_last=True\n    ) #Submission code\n\n        # TODO: Prepare the evaluation DataLoader. Initialize 'DataLoader' with 'eval_dataset',\n        # the appropriate 'batch_size', and 'SequentialSampler' for 'sampler'.\n        # Use 'DataCollatorForSeq2Seq' for 'collate_fn', passing 'tokenizer', padding settings and pad_to_multiple_of to 8, and return_tensors=\"pt\".\n        # Also add drop_last to True.\n\n        data_testloader = DataLoader(\n        eval_dataset,\n        batch_size=self.batch_size,\n        sampler=SequentialSampler(eval_dataset),\n        collate_fn=DataCollatorForSeq2Seq(\n            tokenizer=self.tokenizer, padding=True, pad_to_multiple_of=8, return_tensors=\"pt\"\n        ),\n        drop_last=True\n    ) #Submission code\n\n        return data_trainloader, data_testloader\n\n    def _eval(self, eval_dataloader, epoch: int):\n        avg_loss = 0\n        model.eval()\n        if _is_master_process():\n            eval_progress_bar = tqdm(\n                eval_dataloader, desc=f\"Epoch {epoch + 1} [Evaluation]\", position=0, leave=False)\n        else:\n            eval_progress_bar = eval_dataloader\n\n\n        for batch in eval_progress_bar:\n            with self.ctx:\n                with torch.no_grad():\n                    if not self.is_ddp_training:\n                        outputs = self.model(**batch.to(self.gpu_id))\n                    else:\n                        outputs = self.model(**batch)\n            avg_loss += outputs.loss.item()\n        avg_loss = avg_loss/(len(eval_dataloader))\n        return avg_loss\n\n    def run(self, data_path: str, size_valid_set: float,  seed: int = 123):\n        \"\"\"\n        Run the training process.\n\n        Returns:\n            None\n        \"\"\"\n        # Load dataset\n        train_dataset, eval_dataset = create_datasets(\n            tokenizer=self.tokenizer,\n            max_length=self.max_length,\n            data_path=data_path,\n            size_valid_set=size_valid_set,\n            seed=seed,\n        )\n\n        train_dataloader, eval_dataloader = self.prepare_dataloader(\n            train_dataset, eval_dataset)\n    \n\n        if self.is_ddp_training:\n            self._set_ddp_training()\n\n        self.optimizer = torch.optim.AdamW(\n            self.model.parameters(), lr=learning_rate)\n\n        for epoch in range(self.num_epochs):\n\n            if self.is_ddp_training:\n                train_dataloader.sampler.set_epoch(epoch)\n\n            train_loss = self._run_epoch(train_dataloader, epoch)\n            if self.is_ddp_training:\n                dist.barrier() \n            if _is_master_process() or (epoch == self.num_epochs - 1):\n                eval_loss = self._eval(\n                    eval_dataloader=eval_dataloader, epoch=epoch)\n\n                print(\n                    f\"epoch = {epoch+1} | avg_train_loss = {train_loss} | eval_loss = {eval_loss}\")\n            \n            if _is_master_process():\n                self._save_checkpoint(epoch=epoch+1)\n\ndef load_tokenizer_from_pretrained_model(model_path):\n\n    config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)\n    architecture = config.architectures[0]\n    tokenizer = AutoTokenizer.from_pretrained(\n        model_path, trust_remote_code=True, padding_size='right', device_map={\"\": torch.device(f\"cuda:{0}\")})\n    tokenizer.pad_token = tokenizer.eos_token\n    if _is_master_process():\n        print('Completed to load config & tokenizer')\n\n    if \"Llama\" in architecture:\n        if _is_master_process():\n            print(\"Setting EOS, BOS, UNK, and PAD tokens for LLama tokenizer\")\n        tokenizer.add_special_tokens(\n            {\n                \"eos_token\": \"</s>\",\n                \"bos_token\": \"</s>\",\n                \"unk_token\": \"</s>\",\n            }\n        )\n        tokenizer.pad_token_id = (\n            0  # unk. we want this to be different from the eos token\n        )\n\n    return tokenizer\n\n\ndef _is_master_process():\n    ddp_rank = int(os.environ['RANK'])\n    return ddp_rank == 0\n\n\ndef load_pretrained_model(local_rank, model_path: str = \"\"):\n    # TODO: Load a pretrained AutoModelForCausalLM from the 'model_path'.\n    # Make sure to set 'device_map' to '{\"\": torch.device(f\"cuda:{local_rank}\")}' for DDP training\n    # and trust_remote_code=True.\n\n    model = AutoModelForCausalLM.from_pretrained(\n        model_path, trust_remote_code=True, device_map={\"\": torch.device(f\"cuda:{local_rank}\")}\n    ) #submission code\n\n    # TODO: Create a LoraConfig with the parameters: \n    # r=4, lora_alpha=8, lora_dropout=0.05, bias=\"none\", task_type=\"CAUSAL_LM\"\n    # We will then use the config to initialize a LoraModelForCasualLM with the loaded model.\n\n    lora_config = LoraConfig(r=4, lora_alpha=8, lora_dropout=0.05, bias=\"none\", task_type=\"CAUSAL_LM\")\n     #submission code\n\n    # TODO: Create LoRA model\n\n    model = get_peft_model(model, lora_config) #submission code\n\n    if _is_master_process():\n        model.print_trainable_parameters()\n\n    return model\n\n\nif __name__ == \"__main__\":\n    OUTPUT_DIR = \"checkpoints/\"\n\n    backend = \"nccl\"\n    model_path = 'Qwen/Qwen2.5-1.5B'\n    \n    if os.environ.get(\"DEBUG\"):\n        data_path = 'test_data.json'\n\n    else:\n        data_path = 'alpaca_gpt4_data.json'\n\n\n    size_valid_set = 0.15\n    max_length = 128\n    num_epochs = 3\n    batch_size = 2\n    gradient_accumulation_steps = 8\n\n    learning_rate = 3e-4\n    lr_scheduler_type = 'cosine'\n    num_warmup_steps = 100\n    weight_decay = 0.06\n\n    seed = 0\n    log_freq = 1\n    eval_freq = 150\n\n    distributed_strategy = \"ddp\" if os.environ.get(\"ON_DDP\") else \"no\"\n\n    if distributed_strategy == \"ddp\":\n        # TODO: Initialize the process group for distributed data parallelism with nccl backend.\n        # After that, you should set the 'local_rank' from the environment variable 'LOCAL_RANK'.\n\n        # Initialize the process group\n        init_process_group(backend=backend) #submission code\n        local_rank = int(os.environ['LOCAL_RANK']) #submission code\n        \n    else:\n        os.environ['RANK'] = '0'\n        local_rank = 0\n\n    # Prepare model\n    model = load_pretrained_model(local_rank, model_path=model_path)\n    \n    # Get tokenizer\n    tokenizer = load_tokenizer_from_pretrained_model(model_path=model_path)\n\n    # prepare trainer\n    trainer = Trainer(\n        model=model,\n        num_epochs=num_epochs,\n        max_length=max_length,\n        batch_size=batch_size,\n        gpu_id=local_rank,\n        \n        mixed_precision_dtype=torch.float16 if os.environ.get(\"ON_MP\") else None,\n        \n        tokenizer=tokenizer,\n        output_dir=OUTPUT_DIR,\n        is_ddp_training=True if distributed_strategy == \"ddp\" else False,\n        gradient_accumulation_steps=gradient_accumulation_steps,\n    )\n\n    # set ddp for wraping model\n    # execute trainer\n    trainer.run(\n        data_path=data_path,\n        size_valid_set=size_valid_set,\n        seed=seed,\n        \n    )\n\n    if distributed_strategy == \"ddp\":\n        destroy_process_group()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T12:46:52.953718Z","iopub.execute_input":"2024-12-18T12:46:52.953962Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/684 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0fef258a38ad4fbba9c1c1efb891291e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/3.09G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4ef996d15124ada8763d4d2c895e74a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/138 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96e31e48ca0942ad93588bf60accd0c7"}},"metadata":{}},{"name":"stdout","text":"trainable params: 544,768 || all params: 1,544,259,072 || trainable%: 0.0353\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/7.23k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48f8463109c44abc98c8ab5912a2cd55"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce884c6060344727b91d5753b956e924"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd59819a1472427a808fe905e8ff5f4a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01e4d6b19b3b4ed6bb51881759048274"}},"metadata":{}},{"name":"stdout","text":"Completed to load config & tokenizer\nLoad dataset....\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d76a27d9c628404f849ab948b3b1bf3a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/44201 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c8778e96d64c4c57ac73e17b03c17470"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/7801 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a66d489c54c4e7892f16440c04d2123"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Creating json from Arrow format:   0%|          | 0/8 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"76dcad43816040fbb681ca4f5f490b88"}},"metadata":{}},{"name":"stderr","text":"Epoch 1 [Evaluation]:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2420/3900 [11:14<07:00,  3.52it/s]    ","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"##### Train with sample data","metadata":{}},{"cell_type":"code","source":"# train with sample dataset\n!DEBUG=true python train.py","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##### Challenge 1: Will LoRA enhance inference speed? (5 points)\n\n*[No, Because LoRA mainly focus on reducing computational overhead and storage requirement.] *\n\n##### Challenge 2: Will LoRA improve training speed? (5 points)\n\n*[ Yes. LoRA improves training speed by freezing the majority of the model's parameters and only fine-tuning a small set of low-rank matrices] *","metadata":{}},{"cell_type":"markdown","source":"## Part 2: Mixed precision training\n\nThe paper \"Mixed Precision Training\" is a game-changer in the world of deep learning. It introduces a method that combines different numerical precisions (like 32-bit and 16-bit) during model training. By using lower precision for certain parts of the training process, such as weight updates, we can speed up computations and reduce memory requirements without sacrificing accuracy. This technique leverages the increased computational power of modern GPUs and accelerators to achieve impressive results.\n\n### Implementation\n\n#### Q4: Implement Mixed Precision Training (15 points)\nIn the provided `train.py` file, your objective is to enable mixed precision training. To achieve this, complete the assignment of the `mixed_precision_dtype`, `self.ctx` and `self.gradscaler`. You may have to modify the `_run_batch` and `_run_epoch` using `self.gradscaler` in case you are using `mixed_precision_dtype` of `torch.float16`. If you paid close attention to the coding session during week 6, you should find this task straightforward.\n\nOnce you have carried out these steps, proceed to execute the following cell to train your LoRA model with mixed precision training. You should observe significant speed improvement in training.","metadata":{}},{"cell_type":"markdown","source":"##### Train with sample data","metadata":{}},{"cell_type":"code","source":"# mixed precision training with sample dataset\n!DEBUG=true ON_MP=true python train.py ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Part 3: Distributed Training with DistributedDataParallel\n\nWhen it comes to training large language models, like those used for NLP tasks, the computational requirements can be ridiculously expensive. These models often have billions of parameters and require vast amounts of data to train effectively. This is where distributed training, and more specifically DistributedDataParallel (DDP), comes into play.\n\nTraining large language models on a single GPU can be extremely time-consuming and sometimes outright impossible due to memory limitations. DDP allows us to train these models across multiple GPUs, and even across several machines. This not only speeds up the process but also allows us to train much larger models than would be possible on a single GPU.\n\nBy dividing the model and the dataset across multiple GPUs, each with its own subset of data, we can train in parallel. This significantly reduces the time required to train these large models. Furthermore, the synchronization of model parameters after each forward and backward pass ensures consistency and accuracy across all model replicas.\n\nIn this sections, we will utilize DistributedDataParallel for training large language models. Let's dive in!\n\n### Implementation\n\n#### Q5: Setup environment for DDP (25 points)\n\nIn the provided `train.py` file, your mission is to enable distributed training utilizing the `DistributedDataParallel` (DDP) module from PyTorch. This task involves modifying `distributed_strategy == \"ddp\"`, initializing the process group, establishing the local rank, and filling out the `_set_ddp_training` function. Furthermore, you are required to adapt the `load_pretrained_model` function and the `prepare_dataloader` method to be compatible with DDP training.\n\nOnce you have carried out these steps, complete the `torchrun` command below to execute the following cell to train your LoRA model on Kaggle GPU T4 x2.","metadata":{}},{"cell_type":"code","source":"# distributed training with sample dataset\n\n# TODO Fill in blank \"...\"\n#!DEBUG=true ON_DDP=true ON_MP=true torchrun ... train.py\n\n!DEBUG=true ON_DDP=true ON_MP=true torchrun \\\n    --nproc_per_node=2 \\  # Number of GPUs (T4 x2 in your case)\n    --nnodes=1 \\          # Single-node training\n    --node_rank=0 \\       # Node rank (use 0 for single-node)\n    --master_addr=\"127.0.0.1\" \\  # Master address\n    --master_port=29500 \\  # Master port\n    train.py\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# distributed training with full dataset\n\n# TODO Fill in blank \"...\"\n#!ON_DDP=true ON_MP=true torchrun ... train.py\n\n!ON_DDP=true ON_MP=true torchrun \\\n    --nproc_per_node=2 \\              # Number of GPUs (2 for Kaggle T4 x2)\n    --nnodes=1 \\                      # Single-node training\n    --node_rank=0 \\                   # Node rank (use 0 for single-node)\n    --master_addr=\"127.0.0.1\" \\       # Address of the master node\n    --master_port=29500 \\             # Port for communication\n    train.py\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Inference\n\nOnce the training phase concludes, we can utilize the subsequent code to evaluate our model and generate some instructions. Let's give it a try!","metadata":{}},{"cell_type":"markdown","source":"from inference import generate_inference\n\nmodel_path = \"Qwen/Qwen2.5-1.5B\"\nlora_weights_path = # TODO fill folder path\ninstruction = # TODO  fill instruction\nuser_inp = # TODO: fill input \n\noutputs = generate_inference(instruction=instruction, user_inp=user_inp, model_path=model_path, lora_weights_path=lora_weights_path)\nprint(outputs)","metadata":{"execution":{"iopub.execute_input":"2024-02-05T05:09:21.412123Z","iopub.status.busy":"2024-02-05T05:09:21.411316Z","iopub.status.idle":"2024-02-05T05:10:18.267932Z","shell.execute_reply":"2024-02-05T05:10:18.266861Z","shell.execute_reply.started":"2024-02-05T05:09:21.412074Z"}}},{"cell_type":"code","source":"from inference import generate_inference\n\n# Path to the model\nmodel_path = \"Qwen/Qwen2.5-1.5B\"\n\n# TODO: Fill folder path for the LoRA weights\nlora_weights_path = \"/kaggle/working/Advanced-NLP05/assignment_02\"  # Replace with the actual path where LoRA weights are stored\n\n# TODO: Define the instruction for the model\ninstruction = \"Provide a detailed response to the user's query.\"\n\n# TODO: Provide the user input\nuser_inp = \"Explain the process of distributed training with PyTorch's DistributedDataParallel (DDP).\"\n\n# Generate inference\noutputs = generate_inference(\n    instruction=instruction, \n    user_inp=user_inp, \n    model_path=model_path, \n    lora_weights_path=lora_weights_path\n)\n\nprint(outputs)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}